{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":18648,"databundleVersionId":1026645,"sourceType":"competition"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T05:38:57.321871Z","iopub.execute_input":"2024-11-25T05:38:57.322676Z","iopub.status.idle":"2024-11-25T05:38:57.639677Z","shell.execute_reply.started":"2024-11-25T05:38:57.322625Z","shell.execute_reply":"2024-11-25T05:38:57.638926Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"**fixed seed setting**","metadata":{}},{"cell_type":"code","source":"import torch\nimport random\nimport numpy as np\nimport os\n\nseed = 50\nos.environ['PYTHONHASHSEED']=str(seed)\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.enabled = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T05:38:57.641161Z","iopub.execute_input":"2024-11-25T05:38:57.641526Z","iopub.status.idle":"2024-11-25T05:38:59.066561Z","shell.execute_reply.started":"2024-11-25T05:38:57.641498Z","shell.execute_reply":"2024-11-25T05:38:59.065789Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"**device setting**","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T05:38:59.067532Z","iopub.execute_input":"2024-11-25T05:38:59.067897Z","iopub.status.idle":"2024-11-25T05:38:59.133472Z","shell.execute_reply.started":"2024-11-25T05:38:59.067869Z","shell.execute_reply":"2024-11-25T05:38:59.132417Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"**Data Loader setting**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndata_path = '/kaggle/input/plant-pathology-2020-fgvc7/'\n\ntrain = pd.read_csv(data_path + 'train.csv')\ntest = pd.read_csv(data_path + 'test.csv')\nsubmission = pd.read_csv(data_path + 'sample_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T05:38:59.135576Z","iopub.execute_input":"2024-11-25T05:38:59.135853Z","iopub.status.idle":"2024-11-25T05:38:59.155246Z","shell.execute_reply.started":"2024-11-25T05:38:59.135826Z","shell.execute_reply":"2024-11-25T05:38:59.154440Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain, valid = train_test_split(train,\n                               test_size=0.1,\n                                stratify=train[['healthy', 'multiple_diseases', 'rust', 'scab']],\n                               random_state=50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T05:38:59.156245Z","iopub.execute_input":"2024-11-25T05:38:59.156516Z","iopub.status.idle":"2024-11-25T05:38:59.659009Z","shell.execute_reply.started":"2024-11-25T05:38:59.156489Z","shell.execute_reply":"2024-11-25T05:38:59.658009Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import cv2\nfrom torch.utils.data import Dataset\nimport numpy as np\n\nclass ImageDataset(Dataset):\n    def __init__(self, df, img_dir, transform=None, is_test=False):\n        super().__init__()\n        self.df = df\n        self.img_dir = img_dir\n        self.transform = transform\n        self.is_test = is_test\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_id = self.df.iloc[idx, 0]\n        img_path = self.img_dir + img_id + '.jpg'\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.transform is not None:\n            image = self.transform(image=image)['image']\n        if self.is_test:\n            return image\n        else:\n            label = np.argmax(self.df.iloc[idx, 1:5])\n            return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T05:38:59.660174Z","iopub.execute_input":"2024-11-25T05:38:59.660561Z","iopub.status.idle":"2024-11-25T05:38:59.697823Z","shell.execute_reply.started":"2024-11-25T05:38:59.660532Z","shell.execute_reply":"2024-11-25T05:38:59.697195Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\ntransform_train = A.Compose([\n    A.Resize(450, 650),\n    A.RandomBrightnessContrast(brightness_limit=0.2,contrast_limit=0.2, p=0.3),\n    A.VerticalFlip(p=0.2),\n    A.HorizontalFlip(p=0.5),\n    A.ShiftScaleRotate(\n        shift_limit=0.1,\n        scale_limit=0.2,\n        rotate_limit=30,\n        p=0.3\n    ),\n    A.OneOf([A.Emboss(p=1),\n            A.Sharpen(p=1),\n            A.Blur(p=1)], p=0.3),\n    A.PiecewiseAffine(p=0.3),\n    A.Normalize(),\n    ToTensorV2()\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T05:38:59.698707Z","iopub.execute_input":"2024-11-25T05:38:59.698929Z","iopub.status.idle":"2024-11-25T05:39:00.469100Z","shell.execute_reply.started":"2024-11-25T05:38:59.698907Z","shell.execute_reply":"2024-11-25T05:39:00.468175Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.17). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n/opt/conda/lib/python3.10/site-packages/albumentations/core/validation.py:45: UserWarning: This augmenter is very slow. Try to use ``ElasticTransformation`` instead, which is at least 10x faster.\n  original_init(self, **validated_kwargs)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"transform_test = A.Compose([\n    A.Resize(450, 650),\n    A.Normalize(),\n    ToTensorV2()\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T05:39:00.470141Z","iopub.execute_input":"2024-11-25T05:39:00.470502Z","iopub.status.idle":"2024-11-25T05:39:00.474933Z","shell.execute_reply.started":"2024-11-25T05:39:00.470472Z","shell.execute_reply":"2024-11-25T05:39:00.474117Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"img_dir = '/kaggle/input/plant-pathology-2020-fgvc7/images/'\n\ndataset_train = ImageDataset(train, img_dir=img_dir, transform=transform_train)\ndataset_valid = ImageDataset(valid, img_dir=img_dir, transform=transform_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T05:39:00.476065Z","iopub.execute_input":"2024-11-25T05:39:00.476440Z","iopub.status.idle":"2024-11-25T05:39:00.485122Z","shell.execute_reply.started":"2024-11-25T05:39:00.476398Z","shell.execute_reply":"2024-11-25T05:39:00.484460Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"**Multi Processing setting**","metadata":{}},{"cell_type":"code","source":"def seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T05:39:00.487923Z","iopub.execute_input":"2024-11-25T05:39:00.488630Z","iopub.status.idle":"2024-11-25T05:39:00.495025Z","shell.execute_reply.started":"2024-11-25T05:39:00.488601Z","shell.execute_reply":"2024-11-25T05:39:00.494121Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7f879c6bdaf0>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nbatch_size = 4\n\nloader_train = DataLoader(dataset_train, batch_size=batch_size,\n                         shuffle=True, worker_init_fn=seed_worker,\n                         generator=g, num_workers=2)\nloader_valid = DataLoader(dataset_valid, batch_size=batch_size,\n                         shuffle=False, worker_init_fn=seed_worker,\n                         generator=g, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T05:39:00.495868Z","iopub.execute_input":"2024-11-25T05:39:00.496114Z","iopub.status.idle":"2024-11-25T05:39:00.503564Z","shell.execute_reply.started":"2024-11-25T05:39:00.496089Z","shell.execute_reply":"2024-11-25T05:39:00.502804Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"**Transfer Learning EfficientNet**","metadata":{}},{"cell_type":"code","source":"!pip install efficientnet-pytorch==0.7.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T05:39:00.504608Z","iopub.execute_input":"2024-11-25T05:39:00.504870Z","iopub.status.idle":"2024-11-25T05:39:08.555984Z","shell.execute_reply.started":"2024-11-25T05:39:00.504845Z","shell.execute_reply":"2024-11-25T05:39:08.555067Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: efficientnet-pytorch==0.7.1 in /opt/conda/lib/python3.10/site-packages (0.7.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1) (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1) (1.3.0)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from efficientnet_pytorch import EfficientNet\n\n# pretrained num_classes 1000 -> transfer learning num_classes 4\n\n# method 1 : define num_classes\nmodel = EfficientNet.from_pretrained('efficientnet-b7', num_classes=4)\n\nmodel.to(device)\n\n# method 2 : modify fc layer output\n\"\"\"\nmodel = EfficientNet.from_pretrained('efficientnet-b7')\nmodel._fc = nn.Sequential(\n    nn.Linear(model._fc.in_features, fc.out_features),\n    nn.ReLU(),\n    nn.Dropout(p=0.5),\n    nn.Linear(model._fc.out_features=4)\n)\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T05:39:08.557518Z","iopub.execute_input":"2024-11-25T05:39:08.557913Z","iopub.status.idle":"2024-11-25T05:39:09.798047Z","shell.execute_reply.started":"2024-11-25T05:39:08.557871Z","shell.execute_reply":"2024-11-25T05:39:09.797246Z"}},"outputs":[{"name":"stdout","text":"Loaded pretrained weights for efficientnet-b7\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"\"\\nmodel = EfficientNet.from_pretrained('efficientnet-b7')\\nmodel._fc = nn.Sequential(\\n    nn.Linear(model._fc.in_features, fc.out_features),\\n    nn.ReLU(),\\n    nn.Dropout(p=0.5),\\n    nn.Linear(model._fc.out_features=4)\\n)\\n\""},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"import torch.nn as nn\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.00006, weight_decay=0.0001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T05:39:09.799033Z","iopub.execute_input":"2024-11-25T05:39:09.799352Z","iopub.status.idle":"2024-11-25T05:39:10.480052Z","shell.execute_reply.started":"2024-11-25T05:39:09.799323Z","shell.execute_reply":"2024-11-25T05:39:10.479126Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom tqdm.notebook import tqdm\nepochs = 5\n\n# train loop\nfor epoch in range(epochs):\n    \n    model.train()\n    epoch_train_loss = 0\n    \n    for images, labels in tqdm(loader_train):\n\n        images = images.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n\n        loss = criterion(outputs, labels)\n        epoch_train_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n    print(f'epoch [{epoch+1}/{epochs}] train_loss : {epoch_train_loss/len(loader_train):.4f}')\n    \n    # validation loop\n    model.eval()\n    epoch_valid_loss = 0\n    pred_lists = []\n    true_onehot_list = []\n        \n    with torch.no_grad():\n        for images, labels in loader_valid:\n            images = images.to(device)\n            labels = labels.to(device)\n\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            epoch_valid_loss += loss.item()\n\n            preds = torch.softmax(outputs.cpu(), dim=1).numpy()\n            true_onehot = torch.eye(4, device=device)[labels].cpu().numpy() \n            \n            pred_lists.extend(preds)\n            true_onehot_list.extend(true_onehot)\n\n    print(f'epoch [{epoch+1}/{epochs}] epoch_valid_loss : {epoch_valid_loss/len(loader_valid):.4f} / ROC AUC : {roc_auc_score(true_onehot_list, pred_lists):.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T05:39:10.481227Z","iopub.execute_input":"2024-11-25T05:39:10.481646Z","iopub.status.idle":"2024-11-25T06:18:14.291945Z","shell.execute_reply.started":"2024-11-25T05:39:10.481616Z","shell.execute_reply":"2024-11-25T06:18:14.290846Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/410 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b745b13642aa4afdbed66ad10edd8ce4"}},"metadata":{}},{"name":"stdout","text":"epoch [1/5] train_loss : 0.6942\nepoch [1/5] epoch_valid_loss : 0.2704 / ROC AUC : 0.9405\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/410 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8834902304da458aace6b040cdd05fa7"}},"metadata":{}},{"name":"stdout","text":"epoch [2/5] train_loss : 0.3597\nepoch [2/5] epoch_valid_loss : 0.2525 / ROC AUC : 0.9647\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/410 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25d7ee67750e41ca98b31182374f4f3e"}},"metadata":{}},{"name":"stdout","text":"epoch [3/5] train_loss : 0.2544\nepoch [3/5] epoch_valid_loss : 0.1722 / ROC AUC : 0.9694\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/410 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2af573ba09414c81953092326bd7c8c5"}},"metadata":{}},{"name":"stdout","text":"epoch [4/5] train_loss : 0.1842\nepoch [4/5] epoch_valid_loss : 0.4817 / ROC AUC : 0.9618\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/410 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc5745ca376d4ece8b71b3b23d80c9b7"}},"metadata":{}},{"name":"stdout","text":"epoch [5/5] train_loss : 0.1393\nepoch [5/5] epoch_valid_loss : 0.1665 / ROC AUC : 0.9753\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"dataset_test = ImageDataset(test, img_dir=img_dir,\n                           transform=transform_test, is_test=True)\nloader_test = DataLoader(dataset_test, batch_size=batch_size,\n                        shuffle=False, worker_init_fn=seed_worker,\n                        generator=g, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T06:23:40.155532Z","iopub.execute_input":"2024-11-25T06:23:40.156328Z","iopub.status.idle":"2024-11-25T06:23:40.161594Z","shell.execute_reply.started":"2024-11-25T06:23:40.156289Z","shell.execute_reply":"2024-11-25T06:23:40.160660Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"model.eval()\n\npreds = np.zeros((len(test), 4))\n\nwith torch.no_grad():\n    for i, images in enumerate(loader_test):\n        images = images.to(device)\n        outputs = model(images)\n\n        preds_part = torch.softmax(outputs.cpu(), dim=1).squeeze().numpy()\n        preds[i*batch_size:(i+1)*batch_size] += preds_part","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T06:23:42.671739Z","iopub.execute_input":"2024-11-25T06:23:42.672114Z","iopub.status.idle":"2024-11-25T06:25:59.766993Z","shell.execute_reply.started":"2024-11-25T06:23:42.672080Z","shell.execute_reply":"2024-11-25T06:25:59.765854Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"submission[['healthy', 'mutiple_diseases', 'rust', 'scab']] = preds\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T06:25:59.768932Z","iopub.execute_input":"2024-11-25T06:25:59.769282Z","iopub.status.idle":"2024-11-25T06:25:59.796723Z","shell.execute_reply.started":"2024-11-25T06:25:59.769249Z","shell.execute_reply":"2024-11-25T06:25:59.796007Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}